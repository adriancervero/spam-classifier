{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to build an spam classifier using spam and ham email examples from [Apache SpamAssassin's public datasets](http://spamassassin.apache.org/old/publiccorpus/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, email, email.policy\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import urlextract\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emails using Python email module\n",
    "def load_emails(is_spam):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    path = os.path.join('data', directory)\n",
    "    emails_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), \"rb\") as f:\n",
    "            new_email = email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "            emails_list.append(new_email)\n",
    "    return emails_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails = load_emails(is_spam=True)\n",
    "ham_emails = load_emails(is_spam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 2501)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_emails), len(ham_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have been removed from our list.\n",
      "You will NOT be able to recieve todays picks in the email\n",
      "You will NOT be notified of any new sports pick websites.\n",
      "\n",
      "IF YOU HAVE QUESTIONS ABOUT WHY YOUR ACCOUNT IS EXPIRED,\n",
      "YOUR ACCOUNT WAS CLOSED FOR ONE OF THE FOLLOWING REASONS. \n",
      "\n",
      "1. YOU FAILED TO LOG INTO YOUR ACCCOUNT FOR OVER A MONTH. \n",
      "2. YOUR ACCOUNT WAS FOUND ON A SPAM LIST AND REJECTED. \n",
      "3. THE GIFT ACCOUNT SOMEONE SIGNED YOU UP FOR EXPIRED.\n",
      "\n",
      "If you wish to rejoin please go to the following url:\n",
      "http://www.freewebs.com/registar/\n",
      "\n",
      "YOU DO NOT NEED TO DO ANYTHING TO BE REMOVED FROM THIS eMAIL LIST.\n",
      "THIS IS A ONE TIME MAILING TO NOTIFY YOU THAT, YOU ARE REMOVED.\n",
      "However, you may reply with the word \"remove\" in the subject line\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[1].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how emails are structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2409),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 219),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart/alternative', 1),\n",
       " ('multipart(text/html, text/plain)', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that ham emails are mostly plain text while spam emails has a lot of HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <ilug-admin@linux.ie>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id AA01B43F99\tfor <zzzz@localhost>; Fri, 23 Aug 2002 06:34:02 -0400 (EDT)\n",
      "Received : from phobos [127.0.0.1]\tby localhost with IMAP (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Fri, 23 Aug 2002 11:34:02 +0100 (IST)\n",
      "Received : from lugh.tuatha.org (root@lugh.tuatha.org [194.125.145.45]) by    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g7NAUdZ20252 for    <zzzz-ilug@jmason.org>; Fri, 23 Aug 2002 11:30:39 +0100\n",
      "Received : from lugh (root@localhost [127.0.0.1]) by lugh.tuatha.org    (8.9.3/8.9.3) with ESMTP id LAA21467; Fri, 23 Aug 2002 11:29:43 +0100\n",
      "X-Authentication-Warning : lugh.tuatha.org: Host root@localhost [127.0.0.1]    claimed to be lugh\n",
      "Received : from relay.dub-t3-1.nwcgroup.com    (postfix@relay.dub-t3-1.nwcgroup.com [195.129.80.16]) by lugh.tuatha.org    (8.9.3/8.9.3) with ESMTP id LAA21432 for <ilug@linux.ie>; Fri,    23 Aug 2002 11:29:35 +0100\n",
      "Received : from email.qves.com (unknown [209.63.151.251]) by    relay.dub-t3-1.nwcgroup.com (Postfix) with ESMTP id B39BC70053 for    <ilug@linux.ie>; Fri, 23 Aug 2002 11:11:32 +0100 (IST)\n",
      "Received : from qvp0090 ([169.254.6.21]) by email.qves.com with Microsoft    SMTPSVC(5.0.2195.2966); Fri, 23 Aug 2002 04:10:24 -0600\n",
      "From : \"RankMyPix.com\" <marjani@email2.qves.net>\n",
      "To : ilug@linux.ie\n",
      "Date : Fri, 23 Aug 2002 04:10:14 -0600\n",
      "Message-Id : <2bbe01c24a8d$462eedc0$1506fea9@freeyankeedom.com>\n",
      "MIME-Version : 1.0\n",
      "Content-Type : text/plain; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : 7bit\n",
      "X-Mailer : Microsoft CDO for Windows 2000\n",
      "Thread-Index : AcJKjUYutqtDNiZHR8eiGqDeY7qIUw==\n",
      "Content-Class : urn:content-classes:message\n",
      "X-Mimeole : Produced By Microsoft MimeOLE V6.00.2462.0000\n",
      "X-Originalarrivaltime : 23 Aug 2002 10:10:26.0917 (UTC) FILETIME=[4D660150:01C24A8D]\n",
      "Subject : [ILUG] Join the Web's Fastest Growing Singles Community 11.67\n",
      "Sender : ilug-admin@linux.ie\n",
      "Errors-To : ilug-admin@linux.ie\n",
      "X-Mailman-Version : 1.1\n",
      "Precedence : bulk\n",
      "List-Id : Irish Linux Users' Group <ilug.linux.ie>\n",
      "X-Beenthere : ilug@linux.ie\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "    print(header,':',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[ILUG] Join the Web's Fastest Growing Singles Community 11.67\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails[0]['Subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's split it into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0]* len(ham_emails) + [1]*len(spam_emails)) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2401,), (601,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start defining a function for parsing html to plain text (using Beautiful Soup module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample = html_spam_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = sample.get_content()\n",
    "type(html_to_plain_text(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a function that converts an email into plain text using the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk(): # remember that an email can be multipart\n",
    "        part_type = part.get_content_type()\n",
    "        if not part_type in ('text/plain', 'text/html'):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "            \n",
    "        if part_type == \"text/plain\": # if already plain text\n",
    "            return content\n",
    "        else: \n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Check the REPORTS you would like to receive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Check the Available REPORTS you would like to receive:\n",
      "\n",
      " Keep on TOP of the latest NEWS, Get\n",
      "                Great Special DEALS now...\n",
      "\t\t\t\t It is complimentary, it costs nothing, YOU can QUIT anytime !\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Financial - Stocks - Loans - Mortgage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tFinancial news & Stock market\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                Government & Politics / discussions\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\tCredit Cards &  Mortgage Refinancing / Loans\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "Health - Fitness - Ho\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna use \"NLTK\" for stemming the email's words and \"urlextract\" module in order to find urls in the text. Let's put all this together into a sklearn transformer that we use to convert emails to word counters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "class EmailtoWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_case=True, remove_punctuation=True, replace_urls=True, \n",
    "                 replace_numbers=True, stemming=True):\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for idx, email in enumerate(X):\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if text == None:\n",
    "                print(idx)\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'file': 7, 'number': 6, 'your': 5, 'and': 4, 'or': 4, 'on': 4, 'http': 3, 'to': 3, 'the': 3, 'person': 3, 'desktop': 3, 'a': 2, 'os': 2, 'x': 2, 'link': 2, 'folder': 2, 'laura': 2, 'it': 2, 'with': 2, 'all': 2, 'ani': 2, 'www': 2, 'com': 2, 'url': 1, 'boingbo': 1, 'net': 1, 'date': 1, 'not': 1, 'suppli': 1, 'sixdegress': 1, 'is': 1, 'app': 1, 'that': 1, 'data': 1, 'mine': 1, 'own': 1, 'hard': 1, 'drive': 1, 'tri': 1, 'build': 1, 'between': 1, 'peopl': 1, 'carpent': 1, 'at': 1, 'con': 1, 'wa': 1, 'talk': 1, 'up': 1, 'yesterday': 1, 'look': 1, 'way': 1, 'cool': 1, 'i': 1, 've': 1, 'just': 1, 'download': 1, 'demo': 1, 'play': 1, 'locat': 1, 'similar': 1, 'name': 1, 'revis': 1, 'anywher': 1, 'system': 1, 'show': 1, 'email': 1, 'thread': 1, 'relat': 1, 'view': 1, 'ha': 1, 'sent': 1, 'you': 1, 'regardless': 1, 'of': 1, 'where': 1, 'those': 1, 'are': 1, 'store': 1, 'comput': 1, 'creat': 1, 'dynam': 1, 'self': 1, 'updat': 1, 'project': 1, 'find': 1, 'misfil': 1, 'attach': 1, 'quickli': 1, 'without': 1, 'search': 1, 'navig': 1, 'open': 1, 'messag': 1, 'in': 1, 'one': 1, 'click': 1, 'discuss': 1, '_thank': 1, '_': 1, 'creo': 1, 'sixdegre': 1, 'quicktop': 1, 'bo': 1, 'h': 1, 'vrmgphlfgxpz': 1}),\n",
       "       Counter({'and': 11, 'the': 11, 'a': 11, 'to': 7, 't': 6, 'in': 5, 'all': 4, 'like': 4, 'ask': 4, 'you': 4, 'that': 3, 'is': 3, 'as': 3, 'it': 3, 'they': 3, 'do': 3, 'would': 3, 'of': 3, 'or': 3, 'can': 3, 'queri': 3, 'peopl': 3, 'just': 3, 'bot': 3, 'weather': 3, 'i': 2, 'we': 2, 'help': 2, 'desk': 2, 'doesn': 2, 'are': 2, 'but': 2, 'have': 2, 'mayb': 2, 'there': 2, 'thi': 2, 'than': 2, 'nl': 2, 'databas': 2, 'number': 2, 'even': 2, 'talk': 2, 'noun': 2, 'don': 2, 'type': 2, 'too': 2, 'clunki': 2, 'interfac': 2, 'for': 2, 'true': 2, 'what': 2, 'directli': 2, 'etc': 2, 'origin': 1, 'messag': 1, 'from': 1, 'gari': 1, 'lawrenc': 1, 'murphi': 1, 'garym': 1, 'canada': 1, 'com': 1, 'one': 1, 'thing': 1, 'think': 1, 've': 1, 'learn': 1, 'time': 1, 'work': 1, 'm': 1, 'not': 1, 'sure': 1, 'strictli': 1, 'stuff': 1, 'whole': 1, 'who': 1, 'their': 1, 'right': 1, 'mind': 1, 'use': 1, 'these': 1, 'cool': 1, 'featur': 1, 'isn': 1, 'alway': 1, 'guarante': 1, 'failur': 1, 'strength': 1, 'approach': 1, 'agent': 1, 'im': 1, 'ui': 1, 'find': 1, 'nich': 1, 'applic': 1, 'space': 1, 'prolog': 1, 'base': 1, 'system': 1, 's': 1, 'other': 1, 'later': 1, 'chatterbot': 1, 'helpdesk': 1, 'project': 1, 'shallow': 1, 'red': 1, 'simpler': 1, 'tri': 1, 'jeev': 1, 'veri': 1, 'quickli': 1, 'know': 1, 're': 1, 'robot': 1, 'anneal': 1, 'short': 1, 'truncat': 1, 'ters': 1, 'verb': 1, 'keyword': 1, 'request': 1, 'kind': 1, 'web': 1, 'with': 1, 'googl': 1, 'someon': 1, 'els': 1, 'turn': 1, 'them': 1, 'into': 1, 'link': 1, 'so': 1, 'anyth': 1, 'quick': 1, 'adapt': 1, 'impati': 1, 'forgiv': 1, 'now': 1, 'especi': 1, 'when': 1, 'averag': 1, 'comput': 1, 'user': 1, 'still': 1, 'more': 1, 'numberwpm': 1, 'pain': 1, 'slow': 1, 'ye': 1, 'put': 1, 'way': 1, 'login': 1, 'wake': 1, 'seattl': 1, 'click': 1, 'icon': 1, 'sit': 1, 'on': 1, 'your': 1, 'desktop': 1, 'about': 1, 'situat': 1, 'where': 1, 'listen': 1, 'advis': 1, 'correct': 1, 'interject': 1, 'exampl': 1, 'two': 1, 'discuss': 1, 'trip': 1, 'may': 1, 'trigger': 1, 'mention': 1, 'forecast': 1, 'say': 1, 'without': 1, 'be': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmailtoWordCounterTransformer().fit_transform(X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the word counts, we need to convert them to vectors. For this, we will build another transformer whose fit() method will build the vocabulary (an ordered list of the most common words) and whose transform() method will use the vocabulary to convert word counts to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows=[]\n",
    "        cols=[]\n",
    "        data=[]\n",
    "        \n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = EmailtoWordCounterTransformer().fit_transform(X_train[:2])\n",
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(sample)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114,   4,   3,   2,   3,   6,   7,   4,   5,   2,   1],\n",
       "       [239,  11,  11,  11,   7,   2,   0,   3,   1,   4,   5]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'the': 2,\n",
       " 'a': 3,\n",
       " 'to': 4,\n",
       " 'number': 5,\n",
       " 'file': 6,\n",
       " 'or': 7,\n",
       " 'your': 8,\n",
       " 'all': 9,\n",
       " 'in': 10}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = Pipeline([\n",
    "    ('email_to_wordcount', EmailtoWordCounterTransformer()),\n",
    "    ('wordcount_to_vector', WordCounterToVectorTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 1.32 ms, total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_prepared = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   0.1s\n",
      "CPU times: user 415 ms, sys: 0 ns, total: 415 ms\n",
      "Wall time: 414 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "log_reg = LogisticRegression()\n",
    "scores = cross_val_score(log_reg, X_train_prepared, y_train, cv=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812562421972535"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 98.02%\n",
      "Recall: 95.19%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train_prepared, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
